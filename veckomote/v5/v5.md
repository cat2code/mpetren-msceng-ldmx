## Veckomöte v5

**Deltagare:** Eliot, Lene-Kristian

### Check-in
* Arbetet flyter på tycker jag och det känns som att det finns ett tydligt momentum. Jag har varit på fysicum mkt senaste tiden och jag vill fortsätta såhär som det går nu. Ansikten börjar bli bekanta och jag börjar lära mig namn på folk (och de lär sig mitt namn!). Det är härligt att vara här! :)

### Uppföljning Måldok
* Jag insåg att måldok ska laddas upp på LTH:s exjobbs-hemsida för officiellt godkännande där.
    * Den är nu uppladdad där och inväntar approval från dig, Oxana och Ruth på hemsidan (se mailtråd). 
    * Hemsidan: https://kurser.lth.se/exjobb/application/21605 
* Jag justerade den efter din feedback med de obligatoriska "faktamodifikationerna". 
* Resterande feedback planerar jag inkorporera allt eftersom och dokumentet kan vi hantera som ett internt arbetsdokument. 
    * LTH: "måldok -> projektplan"
    * Overleaf länk: https://sv.overleaf.com/project/691b12336cc62effee4980ac 

### Uppföljning diskussion m. Nairit + "GNN Vs Transformer"
* Sedan förra mötet har jag börjat läsa mig in i GNN och Transformer baserad reconstruction inom HEP (söker runt på Arxive och läser). Samlade mina tankar och idéer inför att snacka med honom (mån -> tis). 
    * Både vad jag läst och vad Nairit tänkte pekar på samma slutsats. 
* Vi "connectade" lunchrummet och snackade i ca 15 min.
    * Han sade att han var upptagen men vi snackade lite kort ändå, det känns som att jag fick ut det jag ville tror jag (jag hade iallafall inte förställt mig något längre än detta iallafall).
        * Vi kmr förmodligen diskutera med honom igen tror jag! Han kmr tillbaka nästa vecka igen. 
    * Han hade läst mitt mail och han ställde lite fler frågor om detaljer kring mitt projekt.  
* I vår vecka-för-vecka planering från förra veckan skrev vi upp i Januari månad "välja arkitektur" ("GNN eller Transformer")
    * Korta svaret: "GNN:s is the way to go" (pga logisk representation av data i modellen), men attention-mechanism kan bli relevant i ett senare skede iallafall (!).
    * Slutsatsen av vår diskussion och min förforskning i fältet pekar nog snarare mot att frågan kanske är konstigt ställd. 
        * För mig verkar det som att konventionen är att transformers inte används som en grundläggande struktur utan snarare som en **katalysator** som kan ge förbättringar i och/eller training time, performance, computational speed, model complexity. 
            * GNN är nästan alltid en starting point. GNNs har en väldigt naturlig representation av datan (eftersom den kan enkelt modelleras som en graf), medan för transformers är det oklart hur man ska mata in datan som input till den. 
            * Det finns däremot modeller som har transformers "rakt igenom", tex ParT för jet tagging. 
        * Det finns exempel på modeller och studier som visat att GNN:s kan effektiviseras genom att man implementerar attention-mechanism i dem när det kommer till edge detection. 
            * (attention/self attention gör att en GNN kan lära sig att identifiera vilka edges (encoding av spatial relation mellan ECal-hits) som är speciellt relevanta). 
                * Nairit verkade positiv till detta och rekommenderade mig att definitivt kolla in attention, när det väl blir relevant. 
                * Han undrade ifall jag skulle utveckla online eller offline reconstruction också, hehe... 
* Nairit rekommenderade att först leka runt med GNN:s (toy model) och att prova använda redskapet i en förenklad version av problemet och försöka se ifall den lyckas lära sig problemet och kan spotta ut något vettigt (proof of concept). 
    * "Start with 1-3 layers (av ECal detektorn) and start adding more and try to get a feeling of how many parameters are needed" 
* **Sammanfattning:** Man ska börja så enkelt som möjligt med ML-modellering, börja med en GNN, lägg till komplexitet allt eftersom och undersök ifall attention kan ge något värdefullt. Utgå från papers och ta inspiration från dem och anpassa till detta problemet (LDMX och LDMX ECal).
* Papers: 
    * MLPF: Machine-learned-particle-flow using graph neural networks https://arxiv.org/pdf/2101.08578
    * Attention-studie i ParT https://arxiv.org/pdf/2512.00210
    * fler...

### Uppföljning CLUE
* Vi får se vad jag har vid mötets start :) . Finns tid över kanske vi kan kika lite kod. 
    * vi kikade kod (tittade åter igen på exempel från .github)

### Punkter till nästa gång
* Lek runt med "toy model" för att känna på GNN lite grand
* Försök att hitta ett så enkelt sätt som möjligt att angripa ECal rechit data med en GNN.
    * "Hur applicerar vi en GNN till ECal"
    * Kika på hur ParticleNet används i LDMX-SW och se ifall det finns saker man kan kopiera därifrån eller ta inspiration i övrigt
        * Annars finns kanske GravNet eller ngt annat...
* Skicka ClusterAnalyzer histogram, ASAP!